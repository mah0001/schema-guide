
# A search engine and recommender system optimized for data {chapter 2}

## A call for better metadata and search algorithms

Data producers have a responsibility to generate comprehensive metadata for their datasets. The administrators and curators of data catalogs are responsible for providing well-documented data, specialized search tools, advice to data users, and for making the metadata as visible as possible to the search engines by implementing search engine optimization (SEO). 

To serve the multiple purposes of improving the usability, visibility, discoverability, and accessibility of data, metadata must be **rich** (detailed and comprehensive) and **structured**. Metadata standards and schemas provide a solution to data curators to improve the comprehensiveness of, and to structure, their metadata. And machine learning and other approaches can be implemented to **augment** (or **enhance**) metadata in a largely automated manner, making them richer. 

Solution is in better algorithms that combine lexical search, semantic search (NLP), filtering, advanced search option (targeted), recommender, enabled by richer and structured metadata. Research, development, and continuous improvement on algorithms and metadata enhancement solutions is needed. Call for investment in open algorithms and models. Metadata standards are there to ingest anbd enable.

## Data cataloguing 

Metadata are primarily intended to make the data **usable** and **discoverable**. 

   - Making data **usable** implies that comprehensive and detailed metadata are provided to data users in formats convenient to them. Depending on the use and user, this format can be HTML, JSON, PDF, XML, or other. 
   - Making data **discoverable** means that the metadata are indexed and searchable in generic and specialized search engines, used to provide useful filtering options (facets) in data catalogs, and made visible by implementing search engine optimization. 
   
To achieve this dual objective of usability and discoverability, structured metadata must be published in on-line data catalogs. We developed this Guide as a complement to our [NADA](https://nada.ihsn.org/) open source cataloguing application, but other on-line data cataloguing applications such as [CKAN](https://ckan.org/) serve similar purposes (although not all will enforce compliance with the metadata standards we recommend). The core features of a modern data catalog are not specific to the NADA application.

The process of publishing data and metadata in a NADA catalog consists of: 

   1. **Documenting the dataset**, i.e. generating structured metadata compliant with one of the supported standards and schemas. The metadata can be produced using a specialized metadata editor (which will often be the case for generating DDI or ISO 19139 metadata), or using a programming language like R or Python. The documentation of the dataset can include procedures of metadata augmentation to make the metadata as rich as possible. 
   2. **Uploading the metadata** in NADA, either using the NADA administrator interface or programmatically using the NADA API (see the NADA documentation). The metadata will then automatically be indexed by the default MySQL engine or, for a better-performing catalog, using thje optional Solr application. NADA will also automatically create and store a DECAT metadata file for the dataset, containing a simplified version of the uploaded metadata. The DECAT metadata is only intended to be accessed by Google and other search engine's web crawlers, and is generated only for search engine optimization purpose.  
   3. **Uploading the resource files, or providing links to these resources**. These resources include all files and links related to the dataset that the catalog administrator wants to make available to users. A brief description of each resource is provided, compliant with the **external resources** schema.
   4. **Select an access policy** to be applied to the dataset, when relevant. This will determine if and how users can access the data (direct access, access from another repository, licensed access, no access, or other). This will typically apply to microdata and geographic datasets, not to other types of data. 


 <!--  ______         _                        -->
 <!-- |  ____|       | |                       -->
 <!-- | |__ ___  __ _| |_ _   _ _ __ ___  ___  -->
 <!-- |  __/ _ \/ _` | __| | | | '__/ _ \/ __| -->
 <!-- | | |  __/ (_| | |_| |_| | | |  __/\__ \ -->
 <!-- |_|  \___|\__,_|\__|\__,_|_|  \___||___/ -->

### Features of a modern data catalog 

We present here some desirable features of a modern on-line data catalog from the perspective of data users and catalog administrators, focusing on the features that are enabled by rich and structured metadata.

#### What users want 

The first thing that users expect from an on-line data catalog is a combination of user-friendly **filters** (**facets**) and **search** tools that will help them identify and access the most relevant datasets. Most users are used to the ease of use and high performance of search engines like Google or Bing. They are also used to the user-friendliness and convenience of commercial platforms like Amazon and others, which allow them to quickly identify products of interest and acquire them in a fast and hassle-free manner. These platforms are simple, predictable, relevant, fast, and reliable. Data catalogs should have similar characteristics. Much can thus be learned from the design of lead search engine and e-commerce sites.

**Filters / facets**

Filters (or *facets*) provide a convenient solution to subset suitable datasets based on a criteria applied to *categorical* metadata elements. For example, in catalog containing datasets from multiple countries, a *country* facet will help users to narrow down a possibly long listing of datasets to a much shorter list. To be effective, filters must be based on metadata elements that have a limited number and a predictable set of categories. The use of controlled vocabularies in data documentation enables such filters. As some metadata elements are specific to a certain data type (e.g., the element "album" is only used for documenting images), the facets in a catalog user interface should be contextual (i.e. different facets will apply for different data types).

<center>
![](./images/catalog_facets_01.JPG){width=100%}
</center>
<br>

Tags and tag groups (which are available in all schemas we recommend) provide much flexibility to implement facets, as we showed in section 1.7. 

**Advanced search**

The search engine of a data catalog must be able to identify relevant datasets based on the user query. A search engine can be <u>lexical</u>, i.e. based on a search for literal matches between the terms in the query and the search engine's index.  words), or <u>semantic</u>, i.e. seeking to find datasets whose metadata are semantically close to the semantic composition of the query (see sections 1.2.3 and 1.8 of the Guide). Ideally, a combination of these two options will be provided in by data catalogs. 

Implementing semantic searchability is somewhat complex; if only a keyword-based search is implemented, efforts should be made to optimize the search engine using tools like Solr or ElasticSearch. Out-of-the box solutions (like those provided by SQL databases) will rarely perform in satisfactory manner. Structured metadata help optimizing search engines, by allowing **boosting** of some of the available metadata elements. For example, a word submitted in a user query should carry more weight when found in the `title` of a dataset than when it is found in a `notes` element; or a country name found in the element `nation` or `reference country` (which indicate the geographic coverage of the data) should carry more weight than when it is found in the description of a variable (where it could for example represent a non-frequent response to a question "place of birth"). Advanced indexing tools like Solr and ElasticSearch provide multiple functionalities to tune search engines to improve the relevancy of the results. See D. Turnbull and J. Berryman (2016) for a detailed description of the tools and methods.

Most search engine will operate based on as single search box. Structured metadata however allows catalog developers to build **advanced search** interfaces (as an complement to a single search box) to allow users to search for keywords in specific metadata elements (e.g., searching for a keyword only in the `authors` element of a document).

A search engine should also be able to process short or long queries, possibly accepting a document as a query (when semantic searchability is implemented).

<center>
![](./images/document_as_query.JPG){width=100%}
</center>
<br>

For keyword-based search engines, keyword suggestions based on a graph of related words may be helpful to users. Graphs of related words are generated by natural language processing (machine learning) models. The implementation of keyword suggestions based on such graphs requires access to an API that can, in real time, provide suggestions based on a query entered by a user. The example below shows a related words graph for the terms "climate change", and the display of the most closely-related terms in a catalog search interface.

<center>
![](./images/related_words_graph.JPG){width=100%}
</center>
<br>

<center>
![](./images/catalog_search_01.JPG){width=100%}
</center>
<br>

A search engine must be fast, and have some tolerance for spelling mistakes. These features themselves are not dependent on the metadata that will be searched; they are functionalities of advanced indexing tools like Solr or ElasticSearch, which are optimized for performance and provide options to enable auto-completion of queries and spell checkers. 

**Results ranking**

A good search engine must be able to not only identify relevant datasets, but also to return the results in a proper order of relevancy, showing the most relevant results at the top of the list. Many users who do not find a relevant response to their query among the first results may decide to search for data elsewhere. The ability for a search engine to return relevant results presented with an optimal rank depends very much on the content and structure of the metadata. A lot of relevance engineering goes into optimizing the ranking of results. This involves tuning of advanced search tools like Solr or ElasticSearch. Well-resourced agencies managing large data catalogs can also enlist data scientists to explore the possibility of using machine learning solutions known as "learn-to-rank". See the section "3.2.2. Improving results ranking" below.

**Return results by data type**

When a data catalog contains data of multiple types, it should also provide an easy way to filter and display the results of user queries by data type. When searching for *US population* for example, a user may be interested only in knowing the total population of the USA, another may need the public use census microdata sample, and a third one may be looking for a publication. Presenting the query results in type-specific tabs (with an option "All"), and/or providing a filter (facet) by type, will allow users to focus on the types of data relevant to them. This is the equivalent of a commercial platform providing results organized by department (e.g., allowing users to search for "keyboard" either in the "music" or in the "electronics" department).

<center>
![](./images/catalog_tabs_01.JPG){width=100%}
</center>
<br>

**Variable-level search and comparison**

Most e-commerce platform provide customers with the possibility to compare products, by displaying their picture and description (i.e. their metadata) side-by-side. For data users, the possibility to compare datasets may also be useful, e.g., to assess the consistency or comparability of a variable or an indicator over time or across sources and countries. Detailed and structured metadata at the variable level (as provided by the DDI and ISO 19110/19139 metadata standards) are required to implement this functionality. 

In the example below, we show how a query for *water* returns not only a list of seven datasets, but also a list of variables in each dataset that match the query.

<center>
![](./images/catalog_variable_view_01.JPG){width=100%}
</center>
<br>

The *variable view* shows that a total of 90 variables match the searched keyword.

<center>
![](./images/catalog_variable_view_02.JPG){width=100%}
</center>
<br>

Users can then select the variables of interest and display their metadata in a format that allows comparison of their respective metadata. Such comparisons benefit from the availability of detailed metadata. For a survey dataset, information on the variable universe, categories, questions and interviewer instructions, and summary statistics will ideally be captured.

<center>
![](./images/catalog_variable_view_03.JPG){width=100%}
</center>
<br>

**Metadata display and export**

Metadata must be displayed and disseminated in a way that will be convenient to users. The display of metadata will be specific to each data type, as each data type uses a specific metadata schema. In on-line catalog, style sheets can be used to control the look and feel of the HTML pages. 

In addition to being displayed in HTML format, metadata should be made available as electronic files in JSON, XML, and possibly PDF format. Structured metadata provides much control and flexibility to automatically generate the JSON and XML files and to format and create the PDF outputs. The JSO and XML files generated by the data catalog must be compliant with the underlying metadata schema and be properly validated. By doing this, the metadata files can easily and reliably be re-used and re-purposed. 

<center>
![](./images/catalog_display_01.JPG){width=100%}
</center>

<br>

**Data and metadata API** 

A modern data catalog application must provide users with access to the data and metadata via an application programming interface (API). The structured metadata allows users to extract specific components of the <u>metadata</u>. For example, a user may want to extract the identifier and the title of all microdata and geographic datasets conducted after year 2000 listed in a data catalog. This can be done easily using an API but would be tedious to do otherwise. Making <u>data</u> accessible via API will allow users to easily acquire the datasets or subset of datasets in an automated and effective manner. This also enables features internal to the catalog, such as dynamic visualizations and data previews (see examples below). Detailed documentation and guidelines on the use of the data and metadata API must be provided to users.

**Digital Object Identifiers (DOI)**

For technical reasons, all datasets in a data catalog must have an identifier that is unique within the catalog. This identifier can be generated automatically, or created based on a pre-defined pattern. But an identifier serves more purposes than satisfying the technical requirement of a software application. For example, identifiers can and should be included in citation requirements for the datasets. 

Ideally, each dataset should have an identifier that is unique not only within a data catalog, but globally (with a uniqueness guaranteed to be permanent). This can be achieved by assigning a Digital Object Identifier (DOI) to a dataset. DOIs can be created in addition to a catalog-specific unique identifier. For more information on the process, conditions, and reasons to generate DOIs for datasets, visit the [DataCite website](https://datacite.org/).   

**Relationships and recommendations**

Not all users will search data catalogs knowing exactly what they should find in it. Some will "explore" catalogs more than search for pre-defined resources. E-commerce platform build complex recommender systems to suggest products to their customers (under headings like "You may also be interested in ..." ; "Products related to this item" ; or "Frequently bought together: ..." ; etc.) Data catalogs do not pursue a similar objective of maximizing sales, but should have a same commitment to bring relevant resources to the attention of their users. To do that, a modern data catalog will display the relationships between entries. The relationships will often be between data of different types (e.g., a *microdata* file can be the input for an *analytical script* that produced a *working paper*; if the data, the script, and the document are all published in a catalog, we would have three related entries).

In some cases, the relationship between resources available in a catalog will be documented in the metadata (e.g., some elements can be used to document the fact that a dataset `is part of`, or `is a new version  of`, or has a different relationship to another identified dataset. But when the relationships are not known and documented, which will most often be the case, the relationships will have to be established in an automated manner. Machine learning tools (like topic models and word embedding models) make it possible to automatically establish the topical or semantic closeness between resources of different types, which can be used to implement a *recommender system* in data catalogs. The image below shows how "related documents" and "related data" can be automatically identified and displayed for a resource (in this case a document).

<center>
![](./images/catalog_related_01.JPG){width=100%}
</center>
<br>

**Clarity in access policies**

The terms of use (ideally provided in the form of a standard license) and the conditions of access to data should be made transparent and visible in the data catalog. The access policy will preferably be provided using a controlled vocabulary, which can be used to enable a facet (filter) as shown in the screenshot below.

<center>
![](./images/catalog_access_policy_01.JPG){width=100%}
</center>
<br>

**Visualizations**

Embedding visualizations can add value to the data catalog. For time series for example, a line chart would be recommended. For images provided with a geo-location, a map indicating the exact location where the image was captured may be useful. For more complex data, other charts can be created. Typically, embedding dynamic charts in a catalog page will require that the data be available via API. A cataloguing application should offer flexibility in the types of charts and maps that can be embedded in a metadata page. The NADA catalog allows catalog adinistrators to generate such visualizations using different tools of their choice. The example below were generated using the open-source [Apache eCharts](https://echarts.apache.org/en/index.html) library.

<br>
*Example: Line chart for a time series*

<center>
![](./images/catalog_visualization_03.JPG){width=100%}
</center>

<br>
*Example: Geo-location of an image*

<center>
![](./images/catalog_visualization_05.JPG){width=100%}
</center>
<br>

**Data preview**

When the data (time series and tabular data, possibly also microdata) are made available via API, the data catalog can also provide a data preview option, and possibly a data extraction option, to the users. Multiple JavaScript tools, some of them open-source, are available to easily embed data grids in catalog pages.

<center>
![](./images/catalog_data_preview_01.JPG){width=80%}
</center>


#### What catalog administrators want

In addition to the features that will make a data catalog meet the expectations of its users, a catalog administrator will expect or appreciate (some of) the following features from a modern data catalog application:

   - Availability of the application as an **open source** application, with detailed technical documentation
   - **Security**, which will among others translate in compatibility with advanced authentication systems, flexibility in the definition of roles/profiles, accreditation of the application by information security experts, regular upgrades and application of security patches, etc.
   - Reasonable IT requirements (e.g., Can the catalog operate on a shared server? How much memory is required?); 
   - Ease of upgrade
   - **Inter-operability** with other catalogs and applications, and compliance with metadata standards. Publishing metadata in multiple data catalogs and hubs is a convenient and effective solution to increase the visibility of data and maximize the service to users. But this process must be automated, in a way that guarantees the proper synchronization of the different catalogs (only one catalog must operate as the "owner" of a dataset; others are only harvesting metadata). Such information exchange requires that the catalogs be inter-operable. Compliance with common formats and metadata standards and schemas enables such exchange of information.
   - **Flexibility** in implementation of data access policies that conform or can be adapted to the specific procedures and protocols of the organization that manages the catalog 
   - Availability of **APIs** to administer the catalog
   - Possibility to feature datasets
   - Easy activation of **usage analytics** (using Google Analytics, Omniture, or other)
   - **Multilingual** capability (internationalization of the code; option for catalog administrators to translate or adapt translations of the software)
   - Advanced search engine
   - Option to implement **semantic search and recommender systems** by exploiting external APIs 
   - Embedded **Search Engine Optimization** (SEO) procedures
   - Option to use widgets to embed custom charts, maps, and data grids in the catalog
   - Possibility to provide feedback and suggestions to the application developers
   





 <!--  __  __            _     _              _                       _              -->
 <!-- |  \/  |          | |   (_)            | |                     (_)             -->
 <!-- | \  / | __ _  ___| |__  _ _ __   ___  | | ___  __ _ _ __ _ __  _ _ __   __ _  -->
 <!-- | |\/| |/ _` |/ __| '_ \| | '_ \ / _ \ | |/ _ \/ _` | '__| '_ \| | '_ \ / _` | -->
 <!-- | |  | | (_| | (__| | | | | | | |  __/ | |  __/ (_| | |  | | | | | | | | (_| | -->
 <!-- |_|  |_|\__,_|\___|_| |_|_|_| |_|\___| |_|\___|\__,_|_|  |_| |_|_|_| |_|\__, | -->
 <!--                                                                          __/ | -->
 <!--                                                                         |___/  -->
 
### Machine learning for improved user experience

We mentioned in Chapter 1 the importance of generating rich metadata, and the possibilities that machine learning approaches offer to enrich metadata. We also mentioned the use of machine learning, and natural language processing (NLP) tools and models in particular, to improve the performance of search engines. Machine learning models enable semantic search engines and recommender systems, and are thus a tool to help users find relevant data. They are also a solution to improve the ranking of results that search engine will find, to ensure that the most relevant ones are brought to the attention of users. The lead search engines (Google, Bing and others) have for many years relied on machine learning. Specialized data catalogs will not have the resources (and do not need) to implement such advanced systems, but should however explore opportunities to exploit machine learning to improve their users' experience. Exploiting machine learning solutions do not necessarily require that catalog administrators build expertise in machine learning or train their own models; data catalogs could make use of external APIs (e.g., APIs that can automatically and in real time translate queries, convert queries into embeddings, and others). Ideally, a global community of practice will developing such APIs (including the training of NLP models) and provide them as a global public good.  

#### Improving discoverability

In 2019 Google announced their implementation of an NLP model named BERT (for Biderectional Encoder Representations from Transformers) as a component of their search engine. Other companies (Amazon, Apple, Microsoft, and others) are developing similar models to improve their search engines. One objective pursued by these companies is to build search engines that can support their digital assistant systems (like Siri, Alexa, Cortana, or Hey Google). These assistants operate on a conversational mode and provide users with answers, not links to resources. Improving NLP models is a continuous and strategic imperative for these companies. And because not all answers can be found in textual resources, Google are also conducting research to develop solutions to [extract answers from tabular data](https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html).

Specialized data catalogs maintained by data centers, statistical agencies, and other data producers and curators still rely almost exclusively on full-text search engines. The search engine embedded in their catalogs look for matches between keywords submitted by the user in the search box (or the stemmed version of them) with keywords found in an index. No attempt is made to understand - and possibly improve - the user's query. This causes problems like the one we illustrated in Chapter 1, where a query for "dutch disease" for example would be wrongly interpreted as a health-related query instead of being understood as an economic concept. 

The administrators of specialized data catalogs do not have the resources to develop and implement the most advanced NLP solutions, and should not need to do so. To help them move from lexical (keyword-based) search systems to semantic search and recommender systems, open solutions, in the form of pre-trained NLP models, open source tools, and open APIs should be developed and published. This would require the development and publishing of global public goods including:

   - **Specialized corpora** and the training of embedding models on these corpora.
   - **Open NLP models and APIs** that data catalogs can exploit to generate embeddings for their metadata (conversion of metadata into numeric vectors for implementation of semantic search and recommender systems), and for providing query suggestions (see example below).
   - **Query parsers**, which can automatically improve/optimize queries and convert them into numeric vectors (for semantic search). Such query parsers would include a pipeline of processes including parsing, stemming and lemmatization, named entity recognition, stop words exclusion, language detection and automated translation, and others.
   - **Guidelines** on the use of tools like Solr, ElasticSearch, and Milvus for implementation of semantic search and recommender systems

Relatively simple models, built on open source tools and publicly-available documents, can provide easy-to-implement solutions. In the example below, we show how the models "understand" the concept of "dutch disease", properly associating it with the relevant economic concepts.

<center>
![](./images/word_graph_dutch_disease.JPG){width=100%}
</center>

#### Improving results ranking

Search engines must be able not only to identify relevant resources, but also to rank and present them to users in an optimal order of relevance. We illustrated the problem in Chapter 1. [Research](https://www.webfx.com/internet-marketing/seo-statistics.html) shows that 75% of search engine users don’t click past the first page.

Data catalogs administrators face two challenges: 

   - improving their ranking in Google and other lead search engines. This is a search engine optimization problem, which is addressed (in part) by enriching metadata and by embedding metadata compliant with the DCAT or schema.org standard in catalog pages. 
   - improving the ranking of results that their own search engines return when a user submits a query. We focus here on this challenge.

Google built is success in 1996 on *PageRank*, a revolutionary approach to rank search results. Since then, Google and other lead search engines have never stopped investing in improving their results ranking approaches (e.g., Google rolled out *RankBrain* in 2015). Advanced ranking methods (which include primary ranking, contextual ranking, and user-specific ranking), are built on machine learning models, referred to as *Learn to Rank* models. [Lucidworks](https://lucidworks.com/post/abcs-learning-to-rank/) provide a clear description of the approach and tools: "Learning to rank (LTR) is a class of algorithmic techniques that apply supervised machine learning to solve ranking problems in search relevancy. In other words, it’s what orders query results. Done well, you have happy employees and customers; done poorly, at best you have frustrations, and worse, they will never return. To perform learning to rank you need access to training data, user behaviors, user profiles, and a powerful search engine such as SOLR. The training data for a learning to rank model consists of a list of results for a query and a relevance rating for each of those results with respect to the query. Data scientists create this training data by examining results and deciding to include or exclude each result from the data set."

Implementing Learn to Rank models may be challenging for data catalog administrators. Building the training dataset, fitting models, and implementing them, requires resources and advanced skills. As an intermediary solution, optimizing the implementation of Solr of ElasticSearch will in many cases contribute significantly (and often sufficiently) to the improvement of results ranking. For more information on the challenge and tools and methods available for relevancy engineering, see D. Turnbull and J. Berryman (2016).

<center>
![](./images/schema_search_ranking.JPG){width=100%}
</center>







 <!--  ______              -->
 <!-- |  ____|             -->
 <!-- | |__ _ __ ___  ___  -->
 <!-- |  __| '__/ _ \/ _ \ -->
 <!-- | |  | | |  __/  __/ -->
 <!-- |_|  |_|  \___|\___| -->

### Free cataloguing tools

The examples we provided in this chapter are taken from our NADA cataloguing application. Other open-source cataloguing applications are available, including CKAN, GeoNetworks, and Dataverse. 

**CKAN**

[CKAN](https://ckan.org/) is a data management system that provides a platform for cataloging, storing and accessing datasets with a rich front-end, full API (for both data and catalog), visualization tools and more. CKAN is an open source software held in trust by Open Knowledge Foundation. It is open and licensed under the GNU Affero General Public License (AGPL) v3.0. CKAN is used by some of the lead open data platforms, such as the [US data.gov](https://www.data.gov/) or the [OCHA Humanitarian Data Exchange](https://data.humdata.org/). CKAN does not require that the metadata comply with any metadata standard (which brings flexibility, but at a cost in terms of discoverability and quality control), but organizes the metadata in the following elements (information extracted from [CKAN on-line documentation](https://docs.ckan.org/en/2.9/)): 

   - *Title*: allows intuitive labeling of the dataset for search, sharing and linking.
   - *Unique identifier*: dataset has a unique URL which is customizable by the publisher.
   - *Groups*: display of which groups the dataset belongs to if applicable. Groups (such as science data) allow easier data linking, finding and sharing among interested publishers and users.
   - *Description*: additional information describing or analyzing the data. This can either be static or an editable wiki which anyone can contribute to instantly or via admin moderation.
   - *Data preview*: preview [.csv] data quickly and easily in browser to see if this is the dataset you want.
   - *Revision history*: CKAN allows you to display a revision history for datasets which are freely editable by users 
   - *Extra fields*: these hold any additional information, such as location data (see geospatial feature) or types relevant to the publisher or dataset. How and where extra fields display is customizable.
   - *Licence*: instant view of whether the data is available under an open license or not. This makes it clear to users whether they have the rights to use, change and re-distribute the data.
   - *Tags*: see what labels the dataset in question belongs to. Tags also allow for browsing between similarly tagged datasets in addition to enabling better discoverability through tag search and faceting by tags.
   - *Multiple formats* (if provided): see the different formats the data has been made available in quickly in a table, with any further information relating to specific files provided inline.
   - *API key*: allows access every metadata field of the dataset and ability to change the data if you have the relevant permissions via API.

The *extra fields* section allows ingestion of structured metadata, which makes it relatively easy to exporting data and metadata from NADA to CKAN. Importing data and metadata from CKAN to NADA is also possible (using the catalog's respective APIs), but with a reduced metadata structure.

**GeoNetworks**

[GeoNetworks](https://geonetwork-opensource.org/) is a cataloguing tool for geographic data and services (not for other types of data), which includes a specialized metadata editor. According to its website, "It provides powerful metadata editing and search functions as well as an interactive web map viewer. It is currently used in numerous Spatial Data Infrastructure initiatives across the world. (...) The metadata editor support ISO19115/119/110 standards used for spatial resources and also Dublin Core format usually used for opendata portals." 

**DataVerse**

The [Dataverse Project](https://dataverse.org/about) is led by the Institute for Quantitative Social Science (IQSS). Dataverse makes use of the DDI Codebook and Dublin Core metadata standards. According to its website, Dataverse "is an open source web application to share, preserve, cite, explore, and analyze research data. (...) The central insight behind the Dataverse Project is to automate much of the job of the professional archivist, and to provide services for and to distribute credit to the data creator." 

"The Institute for Quantitative Social Science (IQSS) collaborates with the Harvard University Library and Harvard University Information Technology organization to make the installation of the Harvard Dataverse Repository openly available to researchers and data collectors worldwide from all disciplines, to deposit data. IQSS leads the development of the open source Dataverse Project software and, with the Open Data Assistance Program at Harvard (a collaboration with Harvard Library, the Office for Scholarly Communication and IQSS), provides user support." 
