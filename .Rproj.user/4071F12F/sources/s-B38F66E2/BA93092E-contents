---
output: html_document
---

# Introduction {-}

**The volume and diversity of socio-economic data available to researchers and analysts have increased considerably over the past decade.** Government agencies, national and international non-governmental organizations, and the private sector, produce large volume of data relevant for research and policy making. These data include microdata from surveys and censuses, databases from administrative systems, remote sensing data and satellite imagery, statistical tables and time series of indicators, documents and reports, collections of photos, and more. In some countries, open data initiatives are being implemented to promote the dissemination, use, and re-purposing of these data.

> In this Guide, we adopt a broad definition of what constitute data. Unit-record level data (microdata) from surveys and censuses, time series and databases of indicators, statistical tables, satellite and remote sensing imagery are *structured, quantitative data*. We also consider collections of textual information (publications, reports, newspaper articles, tweets and other social media posting, and others), and possibly of photos, videos and audio files, as data. These data are not structured, but natural language processing (NLP), image processing, and other machine learning algorithms are available to extract and structure content from such resources. The Guide also covers *data processing and analysis programs and scripts*. These data-related products deserve to be documented and disseminated with the same rigor that applies to data.

**The demand for data is also large, diverse, and changing.** Modern methods and tools allow data of different sources and types to be exploited in an integrated manner. For example, population census and survey data can be combined with satellite imagery to generate high-resolution poverty maps ([ADB, 2021](https://www.adb.org/sites/default/files/publication/695616/mapping-poverty-satellite-imagery-thailand.pdf)); satellite imagery can be combined with census, administrative, and survey data to generate small-area estimates of demographic indicators ([WorldPop](https://www.worldpop.org/)); natural language processing algorithms applied to news streams can generate fine-grained predictors of food insecurity ([Balashankar et al, 2021](https://arxiv.org/pdf/2111.15602.pdf)); and night-light data combined with population estimates can be used to assess unmet electrification needs ([Brian Min and Zachary O'Keeffe, 2021](http://www-personal.umich.edu/~brianmin/HREA/index.html)). Data integration allows analysts to generate more disaggregated and timely insight, and to answer questions that no data source alone could answer. There is thus much value in documenting, packaging, and disseminating data of multiple sources and types.

**This (r)evolution in the supply and demand for data brings opportunities to increase the social and economic value of data.** But data remain difficult to find and use, and many datasets remain under-exploited. Data analysts often devote significant time locating, accessing, and understanding data. A more effective marketplace for data would allow them to devote their attention to analysis.   

**The solution involves the development and maintenance of inter-operable, openly-accessible data catalogs with advanced discovery features.** To foster *discoverability* of data, data catalogs must include advanced search engines and ideally be able to operate as recommender systems. To foster the *visibility* of data, they must embed search engine optimization. To foster the *usability* of data and their *responsible use*, they must provide users with complete and detailed technical documentation, and clear information on the terms of use associated with each dataset. And to ensure efficiency and transparency in research and analysis and in the use of data, they should also include analysis programs and scripts associated with the data (which, in this Guide, we treat as a particular type of "data").

**To achieve these objectives, comprehensive and quality metadata must be provided with the data**. When datasets are published in a searchable catalog, the content that is indexed and made searchable are the metadata, not the data themselves. Metadata include the technical and contextual information related to the data. They can be created manually by data producers and curators, programmatically using specialized algorithms and scripts, or both (as will often be the case for complex datasets). Generating high quality metadata requires skills, resources, and incentives. In too many cases, one or more of these three conditions is not fully met and many datasets are poorly documented and lack discoverability and usability. 

**The content of data catalogs is controlled by the catalog administrator and curator.** This is a major difference with search engines like Google or Bing, which collect and index materials from a vast number of diverse and "external" sources. Data catalogs -- and the search engines embedded in them -- are therefore more similar to commercial platforms than they are from the lead search engines. And much can be learned from these platforms. Indeed, datasets to be disseminated are like products to be sold: they need to be well described, made visible and attractive to customers (including to some customers who may not know exactly what they are looking for), and easily accessible. Structured, rich metadata are an essential component of such imperatives.

The first chapter of this Guide is an advocacy for more and better metadata, enabled by the adoption of different metadata standards and schemas[^1] which are the foundations of the solutions we propose. **Metadata standards** consist of structured and comprehensive lists of *metadata elements* that can be used to organize and store the documentation of data of any type. Each metadata element has a name and a label, and is provided with a description that may include information on its expected content and format, on the mandatory or optional and on the repeatable or non-repeatable character of the element, on suggested controlled vocabularies, and more. Each main data type requires a specific metadata standard or schema.

The development and implementation of **international best practice in data documentation and cataloguing** (data curation) is only one of three component of a work program by the World Bank's Development Data Group aiming to promoting access and responsible use of data. The other two components are (i) the development and implementation of **a search engine optimized for data discovery**, with semantic search capability and able to operate as a recommender system, and (ii) strengthen technical capacity and establish a community of practice around data management and dissemination in low- and middle-income countries. This Guide focuses on the first of these three components. We recommend and describe metadata standards and schemas, and formulate practical recommendations and guidance for their use. We provide examples of compliant metadata generated using R, Python, and specialized metadata editors. We show how these metadata can be published and made searchable in on-line catalogs. 

We use the [NADA](http://nada.ihsn.org/) open source cataloguing application as an example, but the value of standard-compliant metadata would apply to any cataloguing tool. The Guide is not intended to be a substitute to the documentation provided by the custodians of the metadata standards, and it is also not a replacement for the documentation of the tools we refer to, for which specific documentation is available. 

In Chapter 1 of the Guide, we describe the importance of rich and structured metadata. Chapter 2 introduces JSON and XML, the file formats used to describe and exloit metadata standards and schemas. Chapter 3 describes the expected features of a modern data cataloguing solution, designed to optimize discoverability, visibility, and usability of data. Chapters 4 to 12 present the specific standards and schemas we recommend, with examples of their use.

This Guide is intended to be a live document. The standards and schemas it describes, and the related tools, will be updated and upgraded periodically. The Guide is therefore published as an open document in our GitHub repository, and contributions from readers are welcome.  


[^1]: We refer to metadata *standards* when a community or organization is in charge of the development and maintenance of a metadata schema, with formal governance. We use the term metadata *schema* when no formal governance is established.  
