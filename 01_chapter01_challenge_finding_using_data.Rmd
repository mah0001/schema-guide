---
output: html_document
---

# (PART) RATIONALE AND OBJECTIVES  {-}

# The challenge of finding, assessing, accessing, and using data {#chapter01}

Numerous national and international organizations have implemented data-sharing policies to increase access to their data within the research community. The need to share data has become more pressing, with laws requiring public organizations to share data and sponsors of data collection mandating data sharing. However, simply committing to data sharing is not enough. If data is challenging to locate and use, data-sharing policies and regulations will fail to achieve their intended impact. 

Researchers frequently struggle with identifying, assessing, accessing, and utilizing relevant data. Data are obtainable from a wide and growing range of sources, not all of which are readily visible or discoverable. The process of discovering and accessing data can be complex and time-consuming, requiring considerable effort to identify, locate, acquire, comprehend, and manage datasets. The primary obstacles to data visibility, discoverability, and usability include:

- Data often lack comprehensive and optimal metadata. As search engines index metadata, optimizing metadata for discoverability is essential. However, many data producers and custodians lack the expertise, resources, or incentives to document their data assets effectively.
- General search engines like Google or Bing cater to a broad and diverse audience, not a specialized audience requiring assistance in locating, assessing, and acquiring data.
- Search engines integrated into specialized data catalogs have limited capabilities, with most relying on full-text search algorithms often implemented in their default version. Ideally, data catalogs should provide semantic searchability and function as recommender systems.
- Many data catalogs lack visibility due to insufficient proactive search engine optimization.
- Incomplete metadata prevents users from easily evaluating data for their specific purposes.

Enhancing data accessibility and usability will promote the reuse and repurposing of existing data, resulting in a greater return on investment in data collection and creation. In this chapter, we examine several examples that demonstrate the difficulties researchers face when searching for and utilizing data.

## Finding data

Researchers identify and acquire data in various ways. Some rely on personal networks to find and obtain the data they need, which can lead to the use of "convenient" data that may not be the most relevant. Others locate datasets of interest in academic publications, which can be challenging as datasets are often not cited in a consistent or standardized manner. But many researchers search specialized data catalogs or use general search engines to locate relevant datasets or catalogs.

General search engines such as Google and Bing have remarkable capabilities in locating and ranking relevant resources available online, and in some cases, returning instant answers to users' queries. However, these search engines are designed to answer relatively simple queries and may not be able to find or recommend the most relevant data to help researchers answer complex research questions. Additionally, some relevant data may be hidden in locations such as article supplements, attachments to publications, or available as data services and not as datasets, making it challenging to locate them. Other datasets may be available in open data repositories but published with limited metadata, making them mostly invisible to web crawlers. Due to data being scattered across many sources and locations and published with metadata that is not optimized for search engines, users without prior knowledge of the existence and possible location of the data may be poorly served. Google's Dataset Search is one attempt to implement a better search engine for a catalog of datasets, but its performance is also hampered by the paucity of available metadata.

While general search engines are crucial in directing users to relevant catalogs and repositories, specialized online data catalogs and platforms maintained by national or international organizations, academic data centers, data archives, or data libraries are better suited for researchers seeking relevant data. These organizations provide critical data curation services. However, the search algorithms integrated into their data catalogs can sometimes provide unsatisfactory search results due to weaknesses in metadata associated with data resources and the lack of optimization of search indexes and algorithms. Administrators of these catalogs have a responsibility to ensure that their assets are visible in general search engines, and that the search engines and other discovery tools implemented in their catalogs provide an optimized user experience.

The few examples provided below illustrate some of the problems that researchers encounter when searching for data.[2] 

**Example 1: Searching for data on education and income inequality in Kenya using Google**

Suppose an analyst wants to study the correlation between education and inequality in Kenya. Such an analysis requires microdata from household surveys. The challenge here is to locate a certain type of data (microdata), covering not one but a combination of topics (education and income inequality). A Google search for "data on inequality and education in Kenya" returns useful links, but mostly to publications and blogs. Among the top 100 results, none provides a link to a relevant survey dataset.  

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/229517716-ec3e24d8-7303-4233-83e3-1cdf2f63c458.png)
</center>
<br>

A more specific query for "survey microdata on income inequality and education in Kenya" returns more relevant results, including a link to the *Kenya Integrated Household Budget Survey 2015-2016* from the microdata catalog maintained by the Kenya Bureau of Statistics. But the 49 results returned by Google do not include other existing relevant survey datasets that contain variables on education and on household income --such as the *kenya Integrated Household Budget Survey 2005-2006*-- or the other surveys that collected data on education and on proxies of household income such as household expenditure or assets ownership.

**Example 2: Searching for *child malnutrition* in the [US Open Data platform (data.gov)](https://catalog.data.gov)**

This second example illustrates the challenge of finding data based on a query that is not optimized to match the (unknown) metadata indexed in a catalog. The success of a search query depends largely on the user's ability to formulate a clear query, but what may seem clear to a human may not be seen that way by search algorithms. Many search algorithms rely on keyword search, which means they look for an exact match between keywords in a query and keywords in the index (or a stemmed or lemmatized version of the keywords). However, this approach has limitations as synonyms or related terms may not be recognized. A search for data on "child malnutrition" illustrates this. Child malnutrition is typically measured by the percentage of children stunted, wasted, and overweight. Queries for "child malnutrition", "children stunted", "children wasted", and "children overweight" in the U.S. Government’s open data platform (data.gov) return different results, with little overlap between the results of the four queries. 

<br>
<center>
![](./images/search_issue_data_gov1a.JPG){width=90%}
</center>
<br>

<br>
<center>
![](./images/search_issue_data_gov1b.JPG){width=90%}
</center>
<br>

<br>
<center>
![](./images/search_issue_data_gov1c.JPG){width=90%}
</center>
<br>

<br>
<center>
![](./images/search_issue_data_gov1d.JPG){width=90%}
</center>
<br>

Finding indicators appears to be challenging. Finding relevant microdata would be even more problematic. Unless the data curator has included "child malnutrition" as a keyword in the metadata, the search algorithm would have to understand that datasets containing variables *age in months*, *height*, and *weight* are relevant. Metadata augmentation, and the implementation of smarter search algorithms (with semantic capability) are the solutions to this problem.

**Example 3: Searching for *GDP per capita India 2020* in Google**

This third and last example illustrates the challenge of identifying the most relevant and accurate data. Search engine like Google and Bing are increasingly designed to return not just links, but **answers**. Advanced machine learning and natural language processing (NLP) solutions parse the queries and provide a direct answer when possible. ChatBoxes provide answers in conversation mode. Algorithms will make decisions on what data are most suitable. A search for *GDP per capita India 2021* on Google provides an immediate answer in the form of a graph by extracting information published by the World Bank (as of April 2nd, 2023):

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/229374995-ed83ca65-964d-464c-8668-0f390563bef4.png){width=90%}
</center>
<br>

The answer mentions "Sources include: World Bank". The World Bank is indeed probably the source from which the data used by Google are extracted. But the World Bank does not compile national accounts and the official producer of GDP estimates for India is the Ministry of Statistics (MOSPI). MOSPI's website only appears in the 7th page of results (with a link to a PDF document, not to MOSPI's website or data catalog), a page that few users will ever access. For many users, the source of the information may not matter; but for others, a link to the official source - where more metadata and related data may be found - could be more relevant.  

<br>
<center>
![](./images/gdp_india_2_2.JPG){width=90%}
</center>
<br>

The same query on chatGPT returns the following information.

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/229367133-ae5b1319-4682-46ef-8073-856f9e63149d.png){width=90%}
</center>
<br>

Both Google and chatGPT mention the WorldBank as the source. chatGPT provide results that differ from the information available in the World Bank website. And both make a choice on behalf of the user by providing a single indicator, while the World Bank database provides five different indicators that correspond to "GDP per capita".

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/229374548-6036a86f-d8c4-49bd-a916-73d7ad35ee79.png){width=60%}
</center>
<br>

**Example 4: Searching for data on a specific geographic areas**

Suppose a health official looks for data on measles vaccination in Oromia, a state of Ethiopia. Searching for "data on measles vaccination coverage in Oromia' returns 41 results. Searching for "data on measles vaccination coverage in the state of Oromia Ethiopia' returns 36 results. These results do not include the dataset "Ethiopia - Measles Vaccination Coverage" from WorldPop, which provides estimates of measles coverage in the form of raster data covering the entire territory of Ethiopia, at a very disaggregated level (areas of approximately 100m by 100m). This is in part due to sub-optimal search engine optimization of the website, but even if its content had been properly indexed by Google, the term "Oromia" would not have been detected as it does not appear in the description of the dataset. Listing all geographic areas covered by a geographic (or other) dataset would in many cases be very impractical, so better options need to be offered to make datasets discoverable based on the sub-national areas they cover.  

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/230107403-980cfb66-fa5f-4545-a5c9-2e9e4260a18b.png)
</center>
<br>

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/230108342-484d0c25-796f-4262-9487-5b45cf1402d8.png)
</center>
<br>

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/230109748-0dbd345d-e049-443a-a7b3-03d61339f788.png)
</center>
<br>


## Assessing data

Ideally, researchers should have easy access to both relevant datasets and the metadata required to evaluate the data's suitability for their specific purposes. Obtaining a dataset may be time-consuming and sometimes costly. Therefore, users should only invest resources and time in acquiring data that they know is of high quality and relevance.

Assessing a dataset's fitness for a specific purpose necessitates different metadata elements for varying data types and uses. Some metadata elements are straightforward, such as data type, temporal coverage, geographic coverage, scope and universe, and access policy. However, more detailed information may be required. For instance, a survey dataset (microdata) may only be relevant to a researcher if a specific modality of a specific variable has a sufficient number of respondents. If the sample size is minimal, the dataset would not allow for any valid statistical inference. Furthermore, comparability across sources is crucial to many users and uses, so the metadata should provide a detailed description of sampling, universe, variables, concepts, and methods relevant to the data type. A data user may also require information on the frequency of data updates (for time series or panel surveys, for example) and on previous uses of the dataset by the research community.

Even when data access is restricted, detailed metadata should be available openly and through a user-friendly interface.


## Accessing data

Responsible users need to ensure that their use of data conforms to legal, ethical, and technical requirements imposed by the dataset's owner or distributor. To facilitate this, all datasets should be accompanied by detailed information on the attached terms of use, which may be provided using a standard license. For datasets that don't have an option for direct download, the data distributor should implement an easy-to-use procedure. This should provide transparent and user-friendly information on what information the user needs to provide, how long it will take to obtain a response to a request for access to data, what appeal options may be available in case of refusal, how personal information included in the access request forms will be processed and used, what registration process is required, and more.

The modes of access to the data should accommodate different user preferences. These may include bulk download, where the data is provided in multiple "recent" formats, at least one of which is non-proprietary. Alternatively, the data can be made accessible via API, for which detailed documentation (legal and technical) must be provided. A user interface or visualizations may be provided as a mode of access to some types of data, and other options may also be considered.


## Using data

The challenge for data users is not only to discover data, but also to obtain all necessary information to fully understand the data and to use them responsibly and appropriately. A same indicator label, for example *unemployment rate (%)*, can mask significant differences by country, source, and time. The international recommendations for the definition and calculation of *unemployment rate* has changed over time, and not all countries use the same data collection instrument (labor force survey or other) to collect the underlying data. In on-line data dissemination platforms, detailed metadata should therefore always be associated and disseminated with the data. This must be a close association; the relevant metadata will ideally not be more than one click away from the data. This is particularly critical when a platform publishes data from multiple sources that are not fully harmonized.

:::quote
The scope and meaning of labour statistics in general are determined by their source and methodology, and this is certainly true for the unemployment rate. In order to interpret the data accurately, it is crucial to understand what the data convey and how they were collected and constructed, which implies having information on the relevant metadata. The design and characteristics of the data source (typically a labour force survey or similar household survey for the unemployment rate), especially in terms of definitions and concepts used, geographical and age coverage, and reference periods have great implications for the resulting data, making it crucial to take them into account when analysing the statistics. It is also essential to seek information on any methodological changes and breaks in series to assess their impact for trend analysis, and to keep in mind methodological differences across countries when conducting cross-country studies. (From [*Quick guide on interpreting the unemployment rate*](https://ilo.org/wcmsp5/groups/public/---dgreports/---stat/documents/publication/wcms_675155.pdf), International Labour Office – Geneva: ILO, 2019, ISBN : 978-92-2-133323-4 (web pdf)).
:::

Reproducibility: ...


## A FAIR solution

o effectively address the information retrieval challenge, researchers should consider not only the content of the information but also the context within which it is created and the diverse range of potential users who may need it. A foundational element is being mindful of users and their potential interactions with the data and work. To improve search capabilities and increase the visibility of specialized data libraries, a combination of better data curation, search engines, and increased accessibility is necessary. Adhering to the FAIR principles (Findable, Accessible, Interoperable, and Reusable) is an effective approach to data management. (https://doi.org/10.1371/journal.pcbi.1008469)

It is essential to focus on the entire data curation process, from acquisition to dissemination, to optimize data analysis by streamlining the process of finding, assessing, accessing, and preparing data. This requires anticipating user needs and investing in the curation of data for reuse. To ensure data is **findable**, libraries should implement advanced search algorithms and filters, including full-text, advanced, semantic, and recommendation-based search options. Search engine optimization is also crucial for making catalogs more **accessible**. Additionally, multiple modes of data access should be available to improve accessibility, while data should be made **interoperable** to promote data sharing and reusability. Detailed metadata, including fitness for purpose assessments, should be displayed, alongside scripts and permanent availability options, such as a DOI, to promote **reuse**.

In chapter 2, we define the features of a modern catalog in more detail.

[2] The results shown are for a specific date, and are subject to variation over time.
