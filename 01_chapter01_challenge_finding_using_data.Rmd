---
output: html_document
---

# (PART) RATIONALE AND OBJECTIVES  {-}

# The challenge of finding, assessing, accessing, and using data {#chapter01}

Many national and international organizations have adopted data-sharing policies to make their data more accessible to the research community. Sharing data is increasingly becoming a requirement imposed by laws --for data collected by public organizations-- or by the sponsors of data collection activities. However, a commitment to sharing data is not sufficient. If the data are difficult to find and use, data-sharing policies and regulations will not have the expected impact. Researchers often face challenges in finding, assessing, accessing, and using relevant data. Data are available from a large and growing number of sources, not all of which are easily visible and discoverable. The process of locating and accessing data can be time-consuming and complex, requiring significant effort to identify, locate, acquire, understand, and manage datasets. Improving the accessibility and usability of data will encourage the reuse and repurposing of existing data, leading to a greater return on investment in data collection and creation. In this chapter, we explore several examples that highlight the obstacles researchers encounter when searching for and utilizing data.

## Finding data

Researchers identify and acquire data in various ways. Some rely on personal networks to find and obtain the data they need, which can lead to the use of "convenient" data that may not be the most relevant. Others locate datasets of interest in academic publications, which can be challenging as datasets are often not cited in a consistent or standardized manner. But many researchers search specialized data catalogs or use general search engines to locate relevant datasets or catalogs.

General search engines such as Google and Bing have remarkable capabilities in locating and ranking relevant resources available online, and in some cases, returning instant answers to users' queries. However, these search engines are designed to answer relatively simple queries and may not be able to find or recommend the most relevant data to help researchers answer complex research questions. Additionally, some relevant data may be hidden in locations such as article supplements, attachments to publications, or available as data services and not as datasets, making it challenging to locate them. Other datasets may be available in open data repositoriesbut published with limited metadata, making them mostly invisible to web crawlers. Due to data being scattered across many sources and locations and published with metadata that is not optimized for search engines, users without prior knowledge of the existence and possible location of the data may be poorly served. Google's Dataset Search is one attempt to implement a better search engine for a catalog of datasets, but its performance is also hampered by the paucity of available metadata.

While general search engines are crucial in directing users to relevant catalogs and repositories, specialized online data catalogs and platforms maintained by national or international organizations, academic data centers, data archives, or data libraries are better suited for researchers seeking relevant data. These organizations provide critical data curation services. However, the search algorithms integrated into their data catalogs can sometimes provide unsatisfactory search results due to weaknesses in metadata associated with data resources and the lack of optimization of search indexes and algorithms. Administrators of these catalogs have a responsibility to ensure that their assets are visible in general search engines, and that the search engines and other discovery tools implemented in their catalogs provide an optimized user experience.

The few examples provided below illustrate some of the problems that researchers encounter when searching for data.[2] 


**Example 1: Searching for data on education and income inequality in Kenya using Google**

Suppose an analyst wants to study the correlation between education and inequality in Kenya. Such an analysis requires microdata from household surveys. The challenge here is to locate a certain type of data (microdata), covering not one but a combination of topics (education and income inequality). A Google search for "data on inequality and education in Kenya" returns useful links, but mostly to publications and blogs. Among the top 100 results, none provides a link to a relevant survey dataset.  

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/229517716-ec3e24d8-7303-4233-83e3-1cdf2f63c458.png)
</center>
<br>

A more specific query for "survey microdata on income inequality and education in Kenya" returns more relevant results, including a link to the *Kenya Integrated Household Budget Survey 2015-2016* from the microdata catalog maintained by the Kenya Bureau of Statistics. But the 49 results returned by Google do not include other existing relevant survey datasets that contain variables on education and on household income --such as the *kenya Integrated Household Budget Survey 2005-2006*-- or the other surveys that collected data on education and on proxies of household income such as household expenditure or assets ownership.


**Example 2: Searching for *child malnutrition* in the [US Open Data platform (data.gov)](https://catalog.data.gov)**

This second example illustrates the challenge of finding data based on a query that is not optimized to match the (unknown) metadata indexed in a catalog. The success of a search query depends largely on the user's ability to formulate a clear query, but what may seem clear to a human may not be seen that way by search algorithms. Many search algorithms rely on keyword search, which means they look for an exact match between keywords in a query and keywords in the index (or a stemmed or lemmatized version of the keywords). However, this approach has limitations as synonyms or related terms may not be recognized. A search for data on "child malnutrition" illustrates this. Child malnutrition is typically measured by the percentage of children stunted, wasted, and overweight. Queries for "child malnutrition", "children stunted", "children wasted", and "children overweight" in the U.S. Governmentâ€™s open data platform (data.gov) return different results, with little overlap between the results of the four queries. 

<br>
<center>
![](./images/search_issue_data_gov1a.JPG){width=90%}
</center>
<br>

<br>
<center>
![](./images/search_issue_data_gov1b.JPG){width=90%}
</center>
<br>

<br>
<center>
![](./images/search_issue_data_gov1c.JPG){width=90%}
</center>
<br>

<br>
<center>
![](./images/search_issue_data_gov1d.JPG){width=90%}
</center>
<br>

Finding indicators appears to be challenging. Finding relevant microdata would be even more problematic. Unless the data curator has included "child malnutrition" as a keyword in the metadata, the search algorithm would have to understand that datasets containing variables *age in months*, *height*, and *weight* are relevant. Metadata augmentation, and the implementation of smarter search algorithms (with semantic capability) are the solutions to this problem.


**Example 3: Searching for *GDP per capita India 2020* in Google**

This third and last example illustrates the challenge of identifying the most relevant and accurate data. Search engine like Google and Bing are increasingly designed to return not just links, but **answers**. Advanced machine learning and natural language processing (NLP) solutions parse the queries and provide a direct answer when possible. ChatBoxes provide answers in conversation mode. Algorithms will make decisions on what data are most suitable. A search for *GDP per capita India 2021* on Google provides an immediate answer in the form of a graph by extracting information published by the World Bank (as of April 2nd, 2023):

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/229374995-ed83ca65-964d-464c-8668-0f390563bef4.png){width=90%}
</center>
<br>

The answer mentions "Sources include: World Bank". The World Bank is indeed probably the source from which the data used by Google are extracted. But the World Bank does not compile national accounts and the official producer of GDP estimates for India is the Ministry of Statistics (MOSPI). MOSPI's website only appears in the 7th page of results (with a link to a PDF document, not to MOSPI's website or data catalog), a page that few users will ever access. For many users, the source of the information may not matter; but for others, a link to the official source - where more metadata and related data may be found - could be more relevant.  

<br>
<center>
![](./images/gdp_india_2_2.JPG){width=90%}
</center>
<br>

The same query on chatGPT returns the following information.

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/229367133-ae5b1319-4682-46ef-8073-856f9e63149d.png){width=90%}
</center>
<br>

Both Google and chatGPT mention the WorldBank as the source. chatGPT provide results that differ from the information available in the World Bank website. And both make a choice on behalf of the user by providing a single indicator, while the World Bank database provides five different indicators that correspond to "GDP per capita".

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/229374548-6036a86f-d8c4-49bd-a916-73d7ad35ee79.png){width=60%}
</center>
<br>

**Example 4: Searching for data on a specific geographic areas**

Suppose a health official looks for data on measles vaccination in Oromia, a state of Ethiopia. Searching for "data on measles vaccination coverage in Oromia' returns 41 results. Searching for "data on measles vaccination coverage in the state of Oromia Ethiopia' returns 36 results. These results do not include the dataset "Ethiopia - Measles Vaccination Coverage" from WorldPop, which provides estimates of measles coverage in the form of raster data covering the entire territory of Ethiopia, at a very disaggregated level (areas of approximately 100m by 100m). This is in part due to sub-optimal search engine optimization of the website, but even if its content had been properly indexed by Google, the term "Oromia" would not have been detected as it does not appear in the description of the dataset. Listing all geographic areas covered by a geographic (or other) dataset would in many cases be very impractical, so better options need to be offered to make datasets discoverable based on the sub-national areas they cover.  

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/230107403-980cfb66-fa5f-4545-a5c9-2e9e4260a18b.png)
</center>
<br>

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/230108342-484d0c25-796f-4262-9487-5b45cf1402d8.png)
</center>
<br>

<br>
<center>
![image](https://user-images.githubusercontent.com/35276300/230109748-0dbd345d-e049-443a-a7b3-03d61339f788.png)
</center>
<br>


The main obstacles to data visibility, discoverability, and usability are that: 

   - **data often come with limited and sub-optimal metadata**. Search engines will index metadata. Many data custodians lack the expertise, the resources, or the incentives to optimally document their data assets.
   - **search engines embedded in specialized data catalogs have limited capabilities.** Most often, they rely on full-text search and do not provide semantic searchability. Ideally, data catalogs would even go beyond providing semantic searchability and be able to operate as *recommender systems*. To best satisfy the needs of data analysts, search tools embedded in data catalogs should also be able to **distinguish different types of data**, and be **optimized to rank results by relevancy**.
   - **specialized data catalogs lack visibility**, due to a lack of search engine optimization.
   - **metadata may be incomplete** and does not provide the full information that a user may need.
   - **data re-published may be outdated** if not synchronized in quasi-real time. Inter-operability of platforms must be improved; API-based solutions should be implemented, both for data and metadata. And traceability of harvested information should be guaranteed. 


## Assessing data

Ideally, the researcher should not only be provided with links to relevant datasets but also with full documentation of all variables (collected and derived) included in the datasets. The linked resources should include information on the scope and coverage of the data, on the sample size and design, on the comparability of the data with data from other relevant sources, on previous analysis of the data (citations), and more. Reproducible scripts used by other researchers who conducted analysis of the dataset may also be useful.

Are the data really relevant?

Is the sample size large enough? Example: ...

Does it cover the entire universe of interest? And the entire geography of interest?

Is the source reliable

Is the methodology satisfactory

Are data comparable (over time, across sources)

Will the data remain available? Updated?


## Accessing data

Terms of use

How to request access: register, form, ...

Mode of access
- Bulk download
- API
- UI
- Analytical platforms
- Visualizations

File formats
- Proprietary
- Outdated
- Inconvenient (labels, etc)


## Using data

The challenge for data users is not only to discover data, but also to obtain all necessary information to fully understand the data and to use them responsibly and appropriately. A same indicator label, for example *unemployment rate (%)*, can mask significant differences by country, source, and time. The international recommendations for the definition and calculation of *unemployment rate* has changed over time, and not all countries use the same data collection instrument (labor force survey or other) to collect the underlying data. In on-line data dissemination platforms, detailed metadata should therefore always be associated and disseminated with the data. This must be a close association; the relevant metadata will ideally not be more than one click away from the data. This is particularly critical when a platform publishes data from multiple sources that are not fully harmonized.

:::quote
The scope and meaning of labour statistics in general are determined by their source and methodology, and this is certainly true for the unemployment rate. In order to interpret the data accurately, it is crucial to understand what the data convey and how they were collected and constructed, which implies having information on the relevant metadata. The design and characteristics of the data source (typically a labour force survey or similar household survey for the unemployment rate), especially in terms of definitions and concepts used, geographical and age coverage, and reference periods have great implications for the resulting data, making it crucial to take them into account when analysing the statistics. It is also essential to seek information on any methodological changes and breaks in series to assess their impact for trend analysis, and to keep in mind methodological differences across countries when conducting cross-country studies. (From [*Quick guide on interpreting the unemployment rate*](https://ilo.org/wcmsp5/groups/public/---dgreports/---stat/documents/publication/wcms_675155.pdf), International Labour Office â€“ Geneva: ILO, 2019, ISBN : 978-92-2-133323-4 (web pdf)).
:::

Reproducibility: ...


## A FAIR solution

"To solve the information retrieval problem, researchers must therefore think broadly about who needs that information and the context within which it is created, as well as its actual content." "Being mindful of all potential users and how they might need to interact with you and your work is the foundation." (https://doi.org/10.1371/journal.pcbi.1008469)

Combination of better data curation (packaging and documentation), better search engines, and increased visibility of the content of specialized data libraries in general search engines.

The adoption of FAIR principles can help. Data must be Findable, Accessible, Interoperable, and Reusable. Google's Dataset Search is a noteworthy attempt to improve the search engine's performance in finding relevant datasets, but its efficacy is hindered by the paucity of available metadata.

Data curation is essential, not only cataloguing. The whole process, from acquisition to dissemination is important. Objective is to let researcher focus on analysis, by making the process of finding, assessing, accessing, and preparing data as efficient as possible - anticipating theirneeds, and doing the work once to serve many. For sponsors of data collection: invest not only in data production, but also in the curation of data for re-use. 

Findable:
Data catalogs (libraries) with:
- Advanced search algorithms and filters. This includes full text search, advanced search, semantic search, and recommender system. Possibility to enter simple queries, or complex, even a document as a query.
- SEO, as many users will find the catalog via search engines

Accessible:
- Multiple modes of data access

Inter-operable
- Inter-operability

Reusable
- Display of detailed metadata, with possibility to easyly assess fitness for purpose
- Scripts, not only data
- Permanent availability
- DOI

In chapter 2, we define the features of a modern catalog in more detail.

[2] The results shown are for a specific date, and are subject to variation over time.
