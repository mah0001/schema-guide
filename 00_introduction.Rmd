---
output: html_document
---

# Introduction {-}

The volume and the diversity of socio-economic data available to researchers, analysts, and decision makers have increased considerably over the past decade. The demand for data is also growing and diversifying, which brings opportunities to increase the social and economic value of data. The availability of advanced analytical tools and methods and an ever-growing need for more timely and disaggregated data have created new expectations and aspirations from the community. "Data that were initially collected with one intention can be reused for a completely different purpose. (…) Because the potential of data to serve a productive use is essentially limitless, enabling the reuse and repurposing of data is critical if data are to lead to better lives.” ([World Bank, World Development Report 2021](https://www.worldbank.org/en/publication/wdr2021)) But data often remain difficult to find, access, and use. Data analysts devote a disproportionate share of their time to finding and wrangling data, and overly rely on tribal knowledge to identify dataset that best fit their needs. Many valuable datasets remain under-exploited. 

Fostering the responsible dissemination and use of data involves the development and maintenance of inter-operable, openly-accessible data catalogs with advanced discovery features. To improve *discoverability* of data, data catalogs need advanced search algorithms. Ideally, they should be able to also operate as recommender systems. To increase the *visibility* of data, they must implement search engine optimization (SEO). To foster the *usability* of data and their *responsible use*, they must provide users with complete and detailed technical documentation, and with clear information on the terms of use associated with each dataset. And to foster *efficiency and transparency* in analysis and use of the data, reproducible and replicable programs and scripts can be attached to the data.

To achieve these objectives, comprehensive metadata must be provided with the data. Metadata include both technical and contextual information. They can be created manually by data producers and curators, programmatically using specialized algorithms and scripts, or both (as will often be the case for complex datasets). Generating high quality metadata requires skills, resources, and incentives. In too many cases, one or more of these three conditions is not fully met. Many datasets therefore remain poorly documented and lack discoverability, visibility, and usability.[^1]

Chapter 1 of the guide advocates for the adoption of metadata standards and schemas[^2]. for the production of rich and structured metadata. Chapter 2 introduces JSON and XML, the file formats used to describe and exloit metadata standards and schemas. Chapter 3 describes the expected features of a modern data cataloguing solution, designed to optimize discoverability, visibility, and usability of data. Chapters 4 to 13 present the specific standards and schemas we recommend, with examples of their use.

:::note
The definition of data we adopt in this document is broad, although we focus on data used for quantitative analysis of social and economic development issues. These data can be of different *structured* and *unstructured* types. Modern tools and methods allow analysts to analyze these data in an integrated manner to answer questions that no data source alone could answer. For example, population census and survey data can be combined with satellite imagery to generate high-resolution poverty maps [(ADB, 2021)](https://www.adb.org/sites/default/files/publication/695616/mapping-poverty-satellite-imagery-thailand.pdf); satellite imagery can be combined with administrative and survey data to generate small-area estimates of demographic and development indicators [(WorldPop)](https://www.worldpop.org/); natural language processing algorithms applied to news streams can generate fine-grained predictors of food insecurity [(Balashankar et al, 2021)](https://arxiv.org/pdf/2111.15602.pdf); night-light data combined with population estimates can be used to assess unmet electrification needs [(Brian Min and Zachary O’Keeffe, 2021)](http://www-personal.umich.edu/~brianmin/HREA/index.html). There is thus much value in documenting, packaging, cataloguing, and disseminating data of multiple types.

The distinction we make between data types matters to identify the appropriate metadata standards and the tools to be used for their documentation. The data types we cover in this Guide are: 

- Structured (quantitative) data:
  - **Microdata**: the unit-level data on a population of individuals, households, dwellings, facilities, establishments, or other. Microdata can be generated from surveys, censuses, administrative recording systems, or sensors.
  - **Statistical tables**: aggregated statistical information provided in the form of cross-tables, e.g., as published in statistics yearbooks or census reports. Statistical tables are often derived from microdata.
  - **Indicators and time series**: Indicators are summary measures derived from observed facts (often from microdata). When repeated over time at a regular frequency, the indicators form a time series.
   - **Geographic datasets and data services**: Geographic (or geospatial) data identify and depict geographic locations, boundaries, and characteristics of the surface of the earth. They can be provided in the form of datasets (raster or vector data) or data services (web applications).
  
- Unstructured data: 
  - **Text**: A collection of documents (bibliographic resources of any type, such as books, papers, reports, manuals, and other resources consisting of text) form a corpus. Using natural language processing (NLP) techniques, corpora can be converted into structured information. Textual information extracted from social media (such as Tweets or news) can also be submitted to NLP models to extract quantitative data (e.g., by applying classification techniques). 
  - **Images**: Digital images can be processed using machine learning algorithms (of object detection, classification, or other).  
  - **Audio and video recordings**: Speech-to-text algorithms can transform audio and video recordings as text files. They can thus be considered as data in the same way we consider text as data. 
  - **Programs and scripts**: Although they are not data per se, we treat programs and scripts used to edit, transform, tabulate, analyze, model, and visualize data as data-related resources that need to be documented, catalogued, and disseminated in pursuit of transparency and reproducibility of data use.
:::

This Guide is intended to be a live document. The standards and schemas it describes, and the related tools, will be updated and upgraded periodically. The Guide is therefore published as an open document available on GitHub.  

[^1]: Poor discoverability and usability are obviously not the only obstacles to increasing the use of data. Some data may be locked for legal, ethical, commercial, or political reasons. Good metadata is a necessary, not a sufficient condition to improve data use. Research and support is also needed for the development of disclosure limitation tools and methods, for the modernization of legislations, and for promoting openness in data sharing policies.
[^2]: We refer to metadata *standards* when a community or organization is in charge of the development and maintenance of a metadata schema, with formal governance. We use the term metadata *schema* when no formal governance is established.  
