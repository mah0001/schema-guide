---
output: html_document
---

# Introduction {-}

The volume and the diversity of socio-economic data available to researchers, analysts, and decision makers have increased considerably over the past decade. The demand for data is also growing and diversifying. The availability of advanced analytical tools and methods, and an ever-growing need for more timely and disaggregated data, have created new expectations from the data science and statistical community. This brings opportunities to increase the social and economic value of data. "Data that were initially collected with one intention can be reused for a completely different purpose. (…) Because the potential of data to serve a productive use is essentially limitless, enabling the reuse and repurposing of data is critical if data are to lead to better lives.” ([World Bank, World Development Report 2021](https://www.worldbank.org/en/publication/wdr2021)) But data often remain difficult to find, access, and use. Data analysts devote a disproportionate share of their time to finding and wrangling data, and overly rely on tribal knowledge to identify dataset that best fit their needs. Many valuable datasets remain under-exploited. 

:::Note:::
The definition of data we adopt in this document is broad, although we focus on data used for quantitative analysis of social and economic development issues. Modern methods and tools allow data of different sources and types to be exploited in an integrated manner. Population census and survey data can be combined with satellite imagery to generate high-resolution poverty maps ([ADB, 2021](https://www.adb.org/sites/default/files/publication/695616/mapping-poverty-satellite-imagery-thailand.pdf)); satellite imagery can be combined with administrative and survey data to generate small-area estimates of demographic and development indicators ([WorldPop](https://www.worldpop.org/)); natural language processing algorithms applied to news streams can generate fine-grained predictors of food insecurity ([Balashankar et al, 2021](https://arxiv.org/pdf/2111.15602.pdf)); night-light data combined with population estimates can be used to assess unmet electrification needs ([Brian Min and Zachary O'Keeffe, 2021](http://www-personal.umich.edu/~brianmin/HREA/index.html)). The integration of *structured* and *unstructured* data allows analysts to generate more disaggregated and timely insight, and to answer questions that no data source alone could answer. There is thus much value in documenting, packaging, and disseminating data of multiple types.

The distinction we make between data types matters to identify the specific metadata standards and the tools to be used for their transformation, documentation, packaging, analysis, and dissemination. The data types we cover in this Guide are: 

- Structured (quantitative) data:
  - **Microdata**: the unit-level data on a population of individuals, households, dwellings, facilities, establishments, or other. Microdata can be generated from surveys, censuses, administrative recording systems, or sensors.
  - **Statistical tables**: aggregated statistical information provided in the form of cross-tables, e.g., as published in statistics yearbooks or census reports. Statistical tables are often derived from microdata.
  - **Indicators and time series**: Indicators are summary measures derived from observed facts (often from microdata). When repeated over time at a regular frequency, the indicators form a time series.
   - **Geographic datasets and data services**: Geographic (or geospatial) data identify and depict geographic locations, boundaries, and characteristics of the surface of the earth. They can be provided in the form of datasets (raster or vector data) or data services (web applications).
  
- Unstructured data: 
  - **Text**: A collection of documents (bibliographic resources of any type, such as books, papers, reports, manuals, and other resources consisting of text) form a corpus. Using natural language processing (NLP) techniques, corpora can be converted into structured information. Textual information extracted from social media (such as Tweets or news) can also be submitted to NLP models to extract quantitative data (e.g., by applying classification techniques). 
  - **Images**: Digital images can be processed using machine learning algorithms (of object detection, classification, or other).  
  - **Audio and video recordings**: Speech-to-text algorithms can transform audio and video recordings as text files. They can thus be considered as data in the same way we consider text as data. 
  - **Programs and scripts**: Although they are not data per se, we treat programs and scripts used to edit, transform, tabulate, analyze, model, and visualize data as data-related resources that need to be documented, catalogued, and disseminated in pursuit of transparency and reproducibility of data use.
:::

Fostering the responsible dissemination and use of data involves the development and maintenance of inter-operable, openly-accessible data catalogs with advanced discovery features. To improve *discoverability* of data, data catalogs need advanced search algorithms and be able to operate as recommender systems. To increase the *visibility* of data, they must implement search engine optimization (SEO). To foster the *usability* of data and their *responsible use*, they must provide users with complete and detailed technical documentation, andwith  clear information on the terms of use associated with each dataset. And to ensure *efficiency and transparency* in research and analysis and in the use of data, reproducible and replicable analytical programs and scripts can be attached to the data.

To achieve these objectives, comprehensive metadata must be provided with the data. Metadata include both technical and contextual information on the data. They can be created manually by data producers and curators, programmatically using specialized algorithms and scripts, or both (as will often be the case for complex datasets). Generating high quality metadata requires skills, resources, and incentives. In too many cases, one or more of these three conditions is not fully met and many datasets are poorly documented and lack discoverability and usability. 

The first chapter of this Guide advocates for more and better metadata, enabled by the adoption of different metadata standards and schemas[^1] which are the foundations of the solutions we propose. Metadata standards consist of structured and comprehensive lists of *metadata elements* that can be used to organize and store the documentation of data of any type. Each metadata element has a name and a label, and is provided with a description that may include information on its expected content and format, on the mandatory or optional and on the repeatable or non-repeatable character of the element, on suggested controlled vocabularies, and more. Each main data type requires a specific metadata standard or schema.

The development and implementation of international best practice in data documentation and cataloguing (data curation) is only one of three component of a work program by the World Bank's Development Data Group aiming to promoting access and responsible use of data. The other two components are (i) the development and implementation of a search engine optimized for data discovery, with semantic search capability and able to operate as a recommender system, and (ii) strengthen technical capacity and establish a community of practice around data management and dissemination in low- and middle-income countries. This Guide focuses on the first of these three components. We recommend and describe metadata standards and schemas, and formulate practical recommendations and guidance for their use. We provide examples of compliant metadata generated using R, Python, and specialized metadata editors. We show how these metadata can be published and made searchable in on-line catalogs. 

We use the [NADA](http://nada.ihsn.org/) open source cataloguing application as an example, but the value of standard-compliant metadata would apply to any cataloguing tool. The Guide is not intended to be a substitute to the documentation provided by the custodians of the metadata standards, and it is also not a replacement for the documentation of the tools we refer to, for which specific documentation is available. 

We describe in Chapter 1 the importance of rich and structured metadata. Chapter 2 introduces JSON and XML, the file formats used to describe and exloit metadata standards and schemas. Chapter 3 describes the expected features of a modern data cataloguing solution, designed to optimize discoverability, visibility, and usability of data. Chapters 4 to 12 present the specific standards and schemas we recommend, with examples of their use.

This Guide is intended to be a live document. The standards and schemas it describes, and the related tools, will be updated and upgraded periodically. The Guide is therefore published as an open document available on GitHub.  


[^1]: We refer to metadata *standards* when a community or organization is in charge of the development and maintenance of a metadata schema, with formal governance. We use the term metadata *schema* when no formal governance is established.  
